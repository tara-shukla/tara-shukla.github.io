<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tara Shukla</title>
    <link rel="icon" type="image/x-icon" href="src/favicon.ico">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&family=Roboto+Mono:wght@400;500;600&display=swap" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <div class="header">
            <nav class="nav-menu">
                <a href="#home" class="nav-link active">Home</a>
                <a href="#experience" class="nav-link">Experience</a>
                <a href="#projects" class="nav-link">Projects</a>
                <a href="#skills" class="nav-link">Skills</a>
            </nav>
            <button class="mobile-menu-toggle" onclick="toggleMobileMenu()" aria-label="Toggle mobile menu">
                <span class="hamburger-line"></span>
                <span class="hamburger-line"></span>
                <span class="hamburger-line"></span>
            </button>
            <button class="theme-toggle" onclick="toggleTheme()">
                <span id="theme-text">🌙 Dark</span>
            </button>
        </div>

        <!-- Home Section -->
        <section id="home" class="section">
            <div class="profile-section">
                <div class="profile-left">
                    <div class="headshot">
                        <span><img src="src/headshot.png" alt="Tara Shukla"></span>
                    </div>
                    
                    <div class="contact-info">
                        tara.h.shukla (at) gmail (dot) com
                    </div>
                    
                    <h1>Tara Shukla</h1>
                    <div class="title">Student, Machine Learning and Software Engineering</div>

                    <div class="social-links">
                        <a href="https://github.com/tara-shukla" class="social-btn" target="_blank" rel="noopener">
                            <span class="icon-label">
                                <svg class="icon" viewBox="0 0 24 24">
                                    <path d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z"/>
                                </svg>
                                GitHub
                            </span>
                        </a>
                        <a href="https://linkedin.com/in/tara-shukla" class="social-btn" target="_blank" rel="noopener">
                            <span class="icon-label">
                                <svg class="icon" viewBox="0 0 24 24">
                                    <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                                </svg>
                                LinkedIn
                            </span>
                        </a>
                        <a href="https://docs.google.com/document/d/1pJ-PdcncUPr-Kx8yLkudDhl4Xljv6q2i/edit?usp=sharing&ouid=115664221228203447679&rtpof=true&sd=true" class="social-btn" target="_blank" rel="noopener">
                            <span class="icon-label">
                                <svg class="icon" viewBox="0 0 24 24">
                                    <path d="M14,2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18A2,2 0 0,0 20,20V8L14,2M18,20H6V4H13V9H18V20Z"/>
                                </svg>
                                CV
                            </span>
                        </a>
                    </div>
                </div>
                
                <div class="profile-right">
                    <div class="bio">
                        Hi, I’m Tara! I’m a graduate student and machine learning researcher/software engineer. 
                        Last May, I graduated from Princeton University with an A.B. in Computer Science and minors in Statistics/Machine Learning and Korean. I’m currently pursuing my Masters in Computer Science from Cornell University. 
                    </div>
                    
                    <div class="bio">
                        My interests lie in NLP and machine learning; my research and experience revolve around language modeling, AI interpretability and fairness, and multilingual NLP. 
                    </div>

                    <div class="bio">
                        In the past, I’ve worked with the Princeton Digital Learning Center on building multilingual educational language models, and was fortunate enough to be advised by Prof. Lydia Liu for my senior thesis on model judgement capability, and Prof. Brian Kernighan for my junior thesis on cross-transfer multilingual LLM learning. I also have experience in product management and full-stack web development. 
                    </div>
                    
                    <div class="bio">
                        While not working, you can find me hiking or painting!
                    </div>
                    <br>

                    <div class="info-sections">
                        <div class="interests-section">
                            <h3>Interests</h3>
                            <ul class="interests-list">
                                <li>Natural Language Processing</li>
                                <li>AI Fairness & Bias Analysis</li>
                                <li>Multilingual language modeling</li>
                                <li>Full-Stack Development</li>
                            </ul>
                        </div>
                        <br>
                        <div class="education-section">
                            <h3>Education</h3>
                            <div class="education-item">
                                <div class="degree">Master of Engineering in Computer Science</div>
                                <div class="school">Cornell University, Ithaca, NY</div>
                                <div class="year">Expected May 2026</div>
                            </div>
                            <div class="education-item">
                                <div class="degree">A.B. in Computer Science, Minor in Statistics and Machine Learning</div>
                                <div class="school">Princeton University, Princeton, NJ</div>
                                <div class="year">May 2025 • GPA: 3.65/4.0</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

         <!-- Experience Section -->
        <section id="experience" class="section">
            <h2 class="section-title">Work Experience</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-marker"></div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Machine Learning Engineering Intern</h3>
                            <span class="company">McGraw Digital Learning Lab, Princeton University</span>
                            <span class="duration">June 2024 - May 2025</span>
                        </div>
                        <p>Led the development of an LLM chatbot supporting English speakers in Korean language acquisition. Engineered the chatbot using model training, fine-tuning, and evaluation strategies in Python, and full-stack web app development with MongoDB and Docker. Facilitated technical direction through qualitative research and curated bilingual training datasets for pedagogically aware strategies.</p>
                        <div class="timeline-tech">
                            <span class="tech-tag">Python</span>
                            <span class="tech-tag">LLM Training</span>
                            <span class="tech-tag">Docker</span>
                            <span class="tech-tag">Flask</span>
                        </div>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-marker"></div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Lab Teaching Assistant</h3>
                            <span class="company">Princeton Department of Computer Science</span>
                            <span class="duration">January 2024 - Present</span>
                        </div>
                        <p>Worked as a teaching assistant for Programming Systems and Data Structures and Algorithms classes. Provided key support for students in problem-solving techniques, debugging, and understanding key concepts.</p>
                        <div class="timeline-tech">
                            <span class="tech-tag">Teaching</span>
                            <span class="tech-tag">Problem Solving</span>
                            <span class="tech-tag">Student Support</span>
                        </div>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-marker"></div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Full Stack Developer</h3>
                            <span class="company">Princeton TigerPlan</span>
                            <span class="duration">January 2024 - Present</span>
                        </div>
                        <p>Spearheaded full-stack development for a minor-recommendation app to optimize course planning for Princeton undergraduates. Led Agile product development using Jira and integrated UX/UI analysis and user research into product requirements.</p>
                        <div class="timeline-tech">
                            <span class="tech-tag">MongoDB</span>
                            <span class="tech-tag">Python</span>
                            <span class="tech-tag">jQuery</span>
                            <span class="tech-tag">JavaScript</span>
                        </div>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-marker"></div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Product Management Intern</h3>
                            <span class="company">LG U+, Korea</span>
                            <span class="duration">June - August 2023</span>
                        </div>
                        <p>Tracked, analyzed, and optimized customer journeys for LG U+'s global audience. Designed and implemented FAQ sorting system, improved language and accessibility settings, and optimized product pages. Researched market trends and foreign service offerings.</p>
                        <div class="timeline-tech">
                            <span class="tech-tag">Product Management</span>
                            <span class="tech-tag">UX/UI</span>
                            <span class="tech-tag">SEO</span>
                            <span class="tech-tag">Data Analysis</span>
                        </div>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-marker"></div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Front-end Development Intern</h3>
                            <span class="company">Aidha</span>
                            <span class="duration">June - August 2022</span>
                        </div>
                        <p>Utilized Python to conduct Google Analytics data analysis for marketing strategy overhaul. Researched and implemented UX/UI improvements on customer-facing website interface employing wireframing, mockups, and HTML/JavaScript. </p>
                        <div class="timeline-tech">
                            <span class="tech-tag">HTML</span>
                            <span class="tech-tag">JavaScript</span>
                            <span class="tech-tag">UX/UI</span>
                            <span class="tech-tag">Google Analytics</span>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        
        <!-- Projects Section -->
        <section id="projects" class="section">
            <h2 class="section-title">Research Projects</h2>
            <div class="projects-grid">
                
                <div class="project-card">
                    <div class="project-image">
                        <img src="src/thesis/senior_thesis.png" alt="llm_judgment" style="width:100%;height:auto;border-radius:8px;">

                    </div>
                    <div class="project-content">
                        <h3>Leveraging Item Response Theory for LLM Meta-Evaluation </h3>
                        <p>Researches the application of psychometric statistical strategies to evaluate LLM judgment capabilities.</p>
                        
                        <button class="read-more-btn" onclick="toggleReadMore(this)">Read more</button>
                        <div class="project-more" style="display:none;">
                            <i> We rely on LLM judgment capabilities for a range of tasks across different domains, but they show inconsistent ability to evaluate the quality of text inputs. Current static benchmarking methods rely solely on accuracy metrics and are vulnerable to contamination: there’s a critical need for rigorous and interpretable evaluation frameworks for LLMs. </i>
                            <br>
                            <p>
                                My research addresses that need through applying a statistical framework called Item Response Theory (IRT) to evaluate LLM judgment capabilities via simultaneously modeling abilities and task difficulty characteristics. I assessed a wide set of models across three benchmark datasets (MT-Bench, JudgeBench, LLMBar) and essay grading tasks, using both pairwise comparative judgment and zero-shot pointwise numerical scoring. 
                                IRT models (1PL, 2PL, and Graded Response Models) were then applied to analyze performance, estimating latent "judgment capability" (theta), item difficulty, and discrimination.

                                I found that IRT-derived theta estimates provided greater differentiation between model capabilities than accuracy. Also, there is a weak correlation between benchmark performance, challenging the unidimensionality of LLM judgment. 
                                My research provides methodological innovations for benchmark characterization and usage, and offers an interpretable framework for evaluating LLM-as-a-Judge. 
                            </p>
                        </div>
                        
                        <div class="project-tech">
                            <span class="tech-tag">Python</span>
                            <span class="tech-tag">Statistical Modeling</span>
                            <span class="tech-tag">HuggingFace API</span>
                            <span class="tech-tag">LLM Fairness/Efficacy</span>
                        </div>
                        <div class="project-links">
                            <a href="src/thesis/senior_thesis.pdf" target="_blank" class="project-link">Research Paper</a>
                            <a href="https://github.com/tara-shukla/llm_judgment" target="_blank" class="project-link">GitHub</a>
                        </div>
                    </div>
                </div>

                <div class="project-card">
                    <div class="project-image">
                        <img src="src/rev_curse/rev_curse.png" alt="co-occurence bias" style="width:100%;height:auto;border-radius:8px;">
                    </div>
                    <div class="project-content">
                        <h3>Investigating Reversal Generalization in LLMs via Data Augmentation & Contrastive Loss</h3>
                        <p> We investigated the phenomenon of inconsistent reversal generalization in LLMs by employing data augmentation and contrastive loss.</p>

                        <button class="read-more-btn" onclick="toggleReadMore(this)">Read more</button>
                        <div class="project-more" style="display:none;">
                            <i>LLMs exhibit severe ordering bias when trained on directional facts, e.g. they can answer, “A is B” but fail at the equivalent “B is A.” This is called the reversal curse– model biases in bidirectional knowledge retrieval. </i>
                            <br>
                            <p>
                            We reproduced and extended the reversal curse of <a href="https://arxiv.org/abs/2309.12288">Burglund, et.al </a> through using synthetic fact datasets and developing various solutions for improving bidirectional knowledge encoding. We designed two data augmentation pipelines: chain-of-thought reasoning prompts that create causal/deductive inference chains, and negation-based contrastive examples that teach models to reject incorrect mappings. We implemented a symmetric contrastive loss function using an InfoNCE objective to align embeddings of fact pairs with their reversed counterparts during fine-tuning.
                            
                            Our data augmentation strategies showed moderate improvements in reverse-query performance (up to 9% accuracy gain in description-to-person tasks), while maintaining forward-query accuracy – this highlights that targeted data augmentation can improve knowledge generalization. Our contrastive loss degraded performance, highlighting the challenges in multi-objective fine-tuning. 

                            </p>
                        </div>
                        
                        <div class="project-tech">
                            <span class="tech-tag">Python</span>
                            <span class="tech-tag">HuggingFace Transformers</span>
                            <span class="tech-tag">OpenAI API</span>
                            <span class="tech-tag">PyTorch</span>
                        </div>
                        <div class="project-links">
                            <a href="src/rev_curse/rev_curse.pdf" target="_blank" class="project-link">Research Paper</a>
                            <a href="https://github.com/tara-shukla/reversal_curse" target="_blank" class="project-link">GitHub</a>
                        </div>
                    </div>
                </div>

                <div class="project-card">
                    <div class="project-image">
                        <img src="src/llm_demo/llm_demo.png" alt="co-occurence bias" style="width:100%;height:auto;border-radius:8px;">
                    </div>
                    <div class="project-content">
                        <h3>Evaluating Demographic Bias in LLM-Based Essay Grading</h3>
                        <p>Investigation into algorithmic bias across 5 LLMs using 25,000+ student essays. Implemented demographic-blind and demographic-aware testing methodologies to evaluate fairness in automated grading systems.</p>
                        
                        <button class="read-more-btn" onclick="toggleReadMore(this)">Read more</button>
                        <div class="project-more" style="display:none;">
                            <i>
                                LLMs are being integrated into educational systems for assessing student writing: however, they are not robust against bias in scoring. Without understanding LLM judgment bias, automated essay grading systems can  perpetuate or amplify existing educational inequities based on demographic factors. 
                            </i>
                            <br>
                            <p>
                                Using the PERSUADE 2.0 dataset with 25,000+ student essays, we evaluated five LLMs for demographic bias through demographic-blind, demographic-aware, and counterfactual analysis approaches across six demographic variables. We found significant biases in four of five models, with score variations up to 1.9 rubric points when demographic information was included. Our counterfactual analysis highlighted that white students' essays consistently received higher scores, while essays by Hispanic/Latino, Black/African American, and Asian/Pacific Islander students experienced significant score decreases under counterfactual conditions. Our findings highlight fairness concerns for AI-powered educational assessment and demonstrate the need for bias detection and mitigation strategies before widespread deployment of LLM graders.
                            </p>
                        </div>
                        
                        <div class="project-tech">
                            <span class="tech-tag">Python</span>
                            <span class="tech-tag">Statistical Analysis</span>
                            <span class="tech-tag">LLM Evaluation</span>
                            <span class="tech-tag">Bias Detection</span>
                        </div>
                        <div class="project-links">
                            <a href="src/llm_demo/llm_demo.pdf" target="_blank" class="project-link">Research Paper</a>
                            <a href="https://github.com/tara-shukla/llm_demographic_discrim" target="_blank" class="project-link">GitHub</a>
                        </div>
                    </div>
                </div>

                <div class="project-card">
                    <div class="project-image">
                        <img src="src/cross_atn/cross_atn.png" alt="cross-atn" style="width:100%;height:auto;border-radius:8px;">
                        
                    </div>
                    <div class="project-content">
                        <h3>Investigating Interpretability Of Cross-Attention in Neural Machine Translation </h3>
                        <p> Investigates the interpretability of cross-attention mechanisms in neural machine translation models, using traditional MT cross-linguistic alignment as a baseline.</p>

                        <button class="read-more-btn" onclick="toggleReadMore(this)">Read more</button>
                        <div class="project-more" style="display:none;">
                            <i>
                                The internal decision-making of transformer machine translation (MT) models are not interpretable, making it difficult to understand how models arrive at translations. 
                            </i>
                            <br>
                            <p>
                                We aimed to enhance understanding of English-French MT by visualizing cross-attention patterns and comparing them to traditional machine translation cross-linguistic alignment strategies. To do so, we built an analysis pipeline to compare attention distributions in a multilingual transformer model, MarianMT,  to linguistic structures like word alignment, part-of-speech patterns, and syntactic dependency. We found that  cross-attention serves as a distributed (rather than concentrated) alignment proxy, and that different attention heads exhibit patterns that correspond to syntactic, contextual, and semantic dependencies. Our work provided insight into how neural MT systems make cross-linguistic connections, contributing to model interpretability and error detection. 
                            </p>
                        </div>
                        
                        <div class="project-tech">
                            <span class="tech-tag">Python</span>
                            <span class="tech-tag">Hugging Face Transformers</span>
                            <span class="tech-tag">spaCy/NLTK</span>
                            <span class="tech-tag">Matplotlib</span>
                        </div>
                        <div class="project-links">
                            <a href="src/cross_atn/cross_atn_multi.pdf" target ="_blank" class="project-link">Research Paper</a>
                            <a href="https://github.com/tara-shukla/cross_atn_mt" target ="_blank" class="project-link">GitHub</a>
                        </div>
                    </div>
                </div>

                <div class="project-card">
                    <div class="project-image">
                        <img src="src/junior/junior_thesis.png" alt="co-occurence bias" style="width:100%;height:auto;border-radius:8px;">
                    </div>
                    <div class="project-content">
                        <h3>A Comparative Analysis of Multilingual and Monolingual Transformers </h3>
                        <p>Compares the performance of monolingually vs multilingually pretrained transformers in Korean grammar error correction, providing insights into cross-transfer learning across linguistic domains.</p>
                        
                        <button class="read-more-btn" onclick="toggleReadMore(this)">Read more</button>
                        <div class="project-more" style="display:none;">
                            <i>
                                Especially for morphologically complex languages dissimilar to English, there is conflicting evidence on the comparative efficacy of monolingual vs multilingual transformers in language modeling tasks. This study addresses how Transformer model performance varies based on pretraining with single versus multiple source languages for grammar error correction in Korean.
                            </i>
                            <br>
                            <p>
                                I fine-tuned various pretrained monolingual (KoBART, KoBERT, KoT5) and multilingual (mBART, mT5, M2M) Transformer models, along with a statistical baseline (HANSPELL), on a Korean language learner corpus for Grammar Error Correction (GEC). Using Python, KoNLPy, and KAGAS, I measured performance across M2-scoring (precision, recall, F0.5), BLEU, and GLEU metrics. My research found that overall multilingual models outperformed their monolingual counterparts in overall GEC and individual error classification, and all Transformer models significantly surpassed the statistical baseline. While multilingual models showed greater trainability and higher test set recall, BLEU, and GLEU scores, monolingual models like KoT5 exhibited competitive performance in precision and F0.5, and excelled in specific error types, suggesting their ability to capture nuanced single-language grammar. In general my findings highlight multilingual models' potential for leveraging linguistic diversity in underrepresented languages, but underscores the need for tailored tokenization strategies for morphologically rich languages.
                            </p>
                        </div>
                        
                        <div class="project-tech">
                            <span class="tech-tag">Python</span>
                            <span class="tech-tag">HuggingFace Transformers</span>
                            <span class="tech-tag">Multilingual NLP</span>
                        </div>
                        <div class="project-links">
                            <a href="src/junior/junior_thesis.pdf" target="_blank" class="project-link">Research Paper</a>
                            <a href="https://github.com/tara-shukla/multilingual_crosstransfer_learning" target="_blank" class="project-link">GitHub</a>
                        </div>
                    </div>
                </div>

                <div class="project-card">
                    <div class="project-image">
                        <img src="src/compvis/bias_icon.png" alt="co-occurence bias" style="width:100%;height:auto;border-radius:8px;">
                    </div>
                    <div class="project-content">
                        <h3>Co-occurence Bias in Object Detection</h3>
                        <p>Analyzing co-occurrence bias in computer vision object detection. Investigated spurious correlations in COCO dataset using PyTorch and OpenCV, identifying systematic biases affecting model generalization.</p>
                        
                        <button class="read-more-btn" onclick="toggleReadMore(this)">Read more</button>
                        <div class="project-more" style="display:none;">
                            <i>Computer vision models can rely on contextual clues, leading to degraded performance when detecting or classifying objects outside their usual contexts: thus, the presence or absence of commonly co-occurring entities can create bias. </i>
                            <br>
                            <p>
                                We extended Singh et al.'s (2020) contextual bias research to explore whether co-occurrence bias impacts the performance of YOLOv5, an object detection model leveraging locality and bounding boxes. We built data processing pipelines to process images and annotations, identify and isolate object pairs, and create targeted validation sets. We evaluated the model on 125,000+ images via adapting bias metrics for detection tasks. 

                                We found significant co-occurrence biases in the model’s object detection, with some pairs showing up to 2.47x performance degradation. We performed analysis on how object positioning and occlusion patterns can drive these biases; our findings contribute insights for improving detection via dataset curation to mitigate co-occurrence bias. 
                            </p>
                        </div>
                       
                        <div class="project-tech">
                            <span class="tech-tag">PyTorch</span>
                            <span class="tech-tag">OpenCV</span>
                            <span class="tech-tag">Scikit-learn</span>
                            <span class="tech-tag">Computer Vision</span>
                            <span class="tech-tag">Bias Mitigation</span>
                        </div>
                        <div class="project-links">
                            <a href="src/compvis/obj_det_bias.pdf" target="_blank" class="project-link">Research Paper</a>
                            <a href="https://github.com/tara-shukla/cv_detection_bias" target="_blank" class="project-link">GitHub</a>
                        </div>
                    </div>
                </div>

        </section>


        <!-- Accomplishments Section
        <section id="accomplishments" class="section">
            <h2 class="section-title">Research & Academic Accomplishments</h2>
            <div class="accomplishments-grid">
                <div class="accomplishment-card">
                    <div class="accomplishment-icon">🎓</div>
                    <h3>Princeton University Graduate</h3>
                    <p>Computer Science Major, Statistics & ML Minor</p>
                    <span class="date">2025</span>
                </div>

                <div class="accomplishment-card">
                    <div class="accomplishment-icon">🔬</div>
                    <h3>Machine Learning Research</h3>
                    <p>LLM Chatbot Development for Language Learning</p>
                    <span class="date">2024-2025</span>
                </div>

                <div class="accomplishment-card">
                    <div class="accomplishment-icon">📊</div>
                    <h3>Bias Analysis Research</h3>
                    <p>AI Fairness in Educational Assessment</p>
                    <span class="date">2024</span>
                </div>

                <div class="accomplishment-card">
                    <div class="accomplishment-icon">🌍</div>
                    <h3>International Experience</h3>
                    <p>Product Management Internship in Korea</p>
                    <span class="date">2023</span>
                </div>
            </div>
        </section> -->

        <!-- Skills Section -->
        <section id="skills" class="section">
            <h2 class="section-title">Skills and Tools</h2>
            <div class="skills-container">
                <div class="skill-category">
                    <h3>Machine Learning/ AI</h3>
                    <div class="skill-tags">
                        <span class="skill-tag">Python</span>
                        <span class="skill-tag">TensorFlow, PyTorch</span>
                        <span class="skill-tag">HuggingFace, NLTK, spaCy</span>
                        <span class="skill-tag">pandas, scikit-learn, Matplotlib</span>
                        <span class="skill-tag">Cloud computing</span>
                        <span class="skill-tag">Language modeling</span>
                    </div>
                </div>

                <div class="skill-category">
                    <h3>Software Engineering</h3>
                    <div class="skill-tags">
                        <span class="skill-tag">Java</span>
                        <span class="skill-tag">C</span>
                        <span class="skill-tag">Go</span>
                        <span class="skill-tag">C++</span>
                        <span class="skill-tag">JavaScript</span>
                        <span class="skill-tag">MongoDB</span>
                        <span class="skill-tag">SQL</span>
                        <span class="skill-tag">Docker</span>
                        <span class="skill-tag">React, Node.js</span>
                        <span class="skill-tag">Full Stack Development</span>
                        <span class="skill-tag">Databases</span>

                    </div>
                </div>

                <div class="skill-category">
                    <h3>Data Science & Research</h3>
                    <div class="skill-tags">
                        <span class="skill-tag">R</span>
                        <span class="skill-tag">Statistical Analysis</span>
                        <span class="skill-tag">Feature Engineering</span>
                        <span class="skill-tag">Hypothesis Testing</span>
                        <span class="skill-tag">Predictive Modeling</span>
                        <span class="skill-tag">Data visualization</span>
                    </div>
                </div>

            </div>
        </section>

        <div class="footer">
            © 2025 Tara Shukla
        </div>
    </div>

    <script>
        function toggleReadMore(btn) {
            const more = btn.nextElementSibling;
            if (more.style.display === "none" || more.style.display === "") {
                more.style.display = "block";
                btn.textContent = "Show less";
            } else {
                more.style.display = "none";
                btn.textContent = "Read more";
            }
        }

        function toggleTheme() {
            const body = document.body;
            const themeText = document.getElementById('theme-text');
            
            if (body.getAttribute('data-theme') === 'dark') {
                body.removeAttribute('data-theme');
                themeText.textContent = '🌙 Dark';
                localStorage.setItem('theme', 'light');
            } else {
                body.setAttribute('data-theme', 'dark');
                themeText.textContent = '☀️ Light';
                localStorage.setItem('theme', 'dark');
            }
        }

        function toggleMobileMenu() {
            const navMenu = document.querySelector('.nav-menu');
            const mobileToggle = document.querySelector('.mobile-menu-toggle');
            
            navMenu.classList.toggle('mobile-open');
            mobileToggle.classList.toggle('active');
        }

        // Close mobile menu when clicking on a nav link
        document.querySelectorAll('.nav-link').forEach(link => {
            link.addEventListener('click', () => {
                const navMenu = document.querySelector('.nav-menu');
                const mobileToggle = document.querySelector('.mobile-menu-toggle');
                
                navMenu.classList.remove('mobile-open');
                mobileToggle.classList.remove('active');
            });
        });

        // Close mobile menu when clicking outside
        document.addEventListener('click', (e) => {
            const navMenu = document.querySelector('.nav-menu');
            const mobileToggle = document.querySelector('.mobile-menu-toggle');
            
            if (!navMenu.contains(e.target) && !mobileToggle.contains(e.target) && navMenu.classList.contains('mobile-open')) {
                navMenu.classList.remove('mobile-open');
                mobileToggle.classList.remove('active');
            }
        });

        // Smooth scrolling for navigation
        document.querySelectorAll('.nav-link').forEach(link => {
            link.addEventListener('click', (e) => {
                e.preventDefault();
                const targetId = link.getAttribute('href').substring(1);
                const targetSection = document.getElementById(targetId);
                
                // Update active nav link
                document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
                link.classList.add('active');
                
                // Smooth scroll to section
                targetSection.scrollIntoView({ behavior: 'smooth' });
            });
        });

        // Update active nav link on scroll
        window.addEventListener('scroll', () => {
            const sections = document.querySelectorAll('.section');
            const navLinks = document.querySelectorAll('.nav-link');
            
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;
                if (scrollY >= (sectionTop - 200)) {
                    current = section.getAttribute('id');
                }
            });

            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === `#${current}`) {
                    link.classList.add('active');
                }
            });
        });

        // Load saved theme
        const savedTheme = localStorage.getItem('theme');
        if (savedTheme === 'dark') {
            document.body.setAttribute('data-theme', 'dark');
            document.getElementById('theme-text').textContent = '☀️ Light';
        }

        // Header scroll effects
        const header = document.querySelector('.header');
        let lastScrollY = window.scrollY;

        window.addEventListener('scroll', () => {
            const currentScrollY = window.scrollY;
            
            // Add scrolled class for enhanced styling
            if (currentScrollY > 50) {
                header.classList.add('scrolled');
            } else {
                header.classList.remove('scrolled');
            }

            // Hide/show header on scroll (optional effect)
            if (currentScrollY > lastScrollY && currentScrollY > 100) {
                header.style.transform = 'translateY(-100%)';
            } else {
                header.style.transform = 'translateY(0)';
            }
            
            lastScrollY = currentScrollY;
        });

        // Smooth reveal animation for header elements
        const headerElements = document.querySelectorAll('.nav-menu, .theme-toggle');
        headerElements.forEach((element, index) => {
            element.style.opacity = '0';
            element.style.transform = 'translateY(-20px)';
            
            setTimeout(() => {
                element.style.transition = 'all 0.6s ease';
                element.style.opacity = '1';
                element.style.transform = 'translateY(0)';
            }, 200 + (index * 100));
        });
    </script>
</body>
</html>